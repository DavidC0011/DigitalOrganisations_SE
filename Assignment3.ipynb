{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bNgXL9dykymVIuYppyXu-ALz70lOFZud","timestamp":1681206550691}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#hide\n","! [ -e /content ] && pip install -Uqq fastbook\n","import fastbook\n","from fastai.vision.all import *\n","from fastbook import *\n","fastbook.setup_book()"],"metadata":{"id":"HhmY7I5M8VJ8","executionInfo":{"status":"ok","timestamp":1681882422639,"user_tz":-120,"elapsed":57677,"user":{"displayName":"David Cloos","userId":"01050300783638368919"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c014d497-0e57-420a-e1ca-ffe40cd51e89"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hMounted at /content/gdrive\n"]}]},{"cell_type":"markdown","source":["# Introduction to Artificial Neural Networks\n","\n","Please read the introdcution of neuronal networks of the book *Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow*, p. 299-316.\n","\n","**Why have neural networks, even though they were invented early on, only now caught on?**\n","- too little quantity of data to train ANN's on\n","- too little computing power\n","- \n","\n","**What is a percepton and a threshold logic unit (TLU)? Try to define a linear function and a step function of your choice, use some values of your choice and explain what might be the result of the percepton. (maybe using max. two TLU's)**\n","- Perceptron:\n","  - Is an algorithm that is used to define classify given input datapoints. The classification is being performed with the input value, weights, biases and activation function\n","- TLU:\n","  - receives multiple inputs (weights w and bias b) computes a linear funcion (w^t*x+b). And then it applies the step function /activation function and outputs a binary value\n","- Linear Function with threshold = 0 (step function (If z<=0 --> = else 1)) \n","  - *x1*w1+x2*w2-b*\n","  - w1=2, w2=0,3\n","  - b=5\n","    - f(1,3)=-2,1 --> 0\n","    - f(4,6)=4,8--> 1\n","    - f(2,2)=-0,4--> 0\n","\n","\n","**What is a fully connected layer and a output layer? **\n","- fully connected layer:\n"," - every TLU is connected to every input\n","- outpt layer\n","  - Last layer of the ANN where the TLU's outputs the results\n","\n","**Why can we easily combine the equations of multiple instances into a fully connected layer?**\n","- Linear algebra ? \n","\n","**What problem did Marvin Minsky and Seymour Paper highlight that perceptrons could not solve? What is a possible solution?**\n","- Perceptron could not solve simple problems such as XOR classification problems. \n","  - However multilayer perceptron solve this issue\n","\n","**What is a deep neuronal network? What are hidden layers? What means feedforward neural network (FNN).**\n"," - Hidden layers\n","  - All layers that are no input nor output layers\n","- Deep neural network = An NN that contains 100's of hidden layers\n","\n","**Try to explain how backpropagation works! (In Addition, you can have a look to the following example, which tries manually to compute the backprogation of a simple linear network.** https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ OR you can also read through the Google Colab [04_mnist_basics.ipynb](https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb#scrollTo=t1DK6o-gckCy))\n","1. Forward pass. In this phase we feed the inputs through the network, make a prediction and measure its error with respect to the true label.\n","2. Backward pass. We propagate the gradients of the error with respect to each one of the weights backward from the output layer to the input layer.\n","3. Gradient descent step. We slightly tweak the connection weights in the network by taking a step in the opposite direction of the error gradients\n","\n","- mini batch = divide training set into randomly selected smaller chunks (because of computational reasons)\n","- Backpropagation calculates derivates of a ANN, to train a feedforwards neural network we use stochastic gradient descent --> find the lowests point of a function so that the error is minimized\n","- A loss function (e.g. mean squared error) represents the distance between the prediction and the true labels. Backpropagation let's us calculate the gradient of the loss function with respect to each of the networks weights. With a lot of iterations each weights is updated individually to reduce the loss function\n","- Chain rule of calculus is used to decrease the costs with respect to the weights of a given instance of a training set (e.g. identifying hand written 2's). The same then is applied to decrease the costs with respect due to the biases\n","\n","\n","\n","**Why do we need activation functions, wouldn't it be easier just using linear functions?**\n"," - Linear function can only be applied when the dataset can be distinguished in two classes with a linear function. However most datasets are much more complicated to distinguish two or more classes. Most of the time a linear function is not suitable then. Therefore, we need activation functions (such as ReLU or Sigmoid) which can be tweaked such that a function is not linear \n","\n","## Ideas for the learning portfolio: \n","\n","1) For example, you could train a single TLU to classify iris flowers based on petal length and width in the !!!pyTorch!! environment.\n","\n","2) You could add to our king county housepricing ML project a neuronal network and compare it to the other models. "],"metadata":{"id":"_Rdj49uwjuoU"}},{"cell_type":"code","source":["from sklearn import datasets\n","iris = datasets.load_iris()"],"metadata":{"id":"4tF8YDV3T91n","executionInfo":{"status":"ok","timestamp":1681882422640,"user_tz":-120,"elapsed":18,"user":{"displayName":"David Cloos","userId":"01050300783638368919"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# A traditional approach: training a digit classifier and learning pyTorch tensors.\n","\n","For this assignment, I ask you to read the Google Colab [04_mnist_basics.ipynb](https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb#scrollTo=t1DK6o-gckCy) to the beginning of the chapter *Stochastic Gradient Descent (SGD)*. \n","\n","First, try to summarize what we know about pyTorch tensors by trying to predict whether we have a 1 or a 7 in the MNIST dataset using a traditional rule-based programming approach. Therefore use pyTorch tensors for the entire tasks and fulfill the following steps:\n","\n","1) Randomly split the MNIST dataset (1 and 7) into a training dataset and a test dataset in a ratio of 80:20.\n","\n","2) Instead of using an optimal 1 or 7 with the mean over the training dataset, try to calculate the sum of the distances to all instances in the training set for each instance in the test dataset. You can use the L2 norm. \n","\n","3) For each instance in the test set, decide if it is a 1 or 7 and calculate the precision.\n","\n","Do we get a similar good result?\n"],"metadata":{"id":"h6OwXNEeed93"}},{"cell_type":"code","source":["#hide\n","! [ -e /content ] && pip install -Uqq fastbook\n","import fastbook\n","fastbook.setup_book()\n","\n","#hide\n","from fastai.vision.all import *\n","from fastbook import *\n","\n","matplotlib.rc('image', cmap='Greys')"],"metadata":{"id":"23QX3rpnsrYa","executionInfo":{"status":"ok","timestamp":1681882430112,"user_tz":-120,"elapsed":7482,"user":{"displayName":"David Cloos","userId":"01050300783638368919"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","#1) Randomly split the MNIST dataset (1 and 7) into a training dataset and a test dataset in a ratio of 80:20.\n","path = untar_data(URLs.MNIST)\n","Path.BASE_PATH = path\n","path.ls()\n","\n","ones = ((path/'training'/'1').ls() + (path/'testing'/'1').ls()).sorted()\n","sevens = ((path/'training'/'7').ls() + (path/'testing'/'7').ls()).sorted()\n","\n","train_ones, test_ones = train_test_split(ones, test_size=0.2, random_state=42)\n","train_sevens, test_sevens = train_test_split(sevens, test_size=0.2, random_state=42)\n","\n","#convert images to pytorch tensor (numerical values)\n","train_ones_tensor = [tensor(Image.open(o)) for o in train_ones]\n","stacked_train_ones_tensor = torch.stack(train_ones_tensor).float()/255\n","\n","test_ones_tensor = [tensor(Image.open(o)) for o in test_ones]\n","stacked_test_ones_tensor = torch.stack(test_ones_tensor).float()/255\n","\n","train_sevens_tensor = [tensor(Image.open(o)) for o in train_sevens]\n","stacked_train_sevens_tensor = torch.stack(train_sevens_tensor).float()/255\n","\n","test_sevens_tensor = [tensor(Image.open(o)) for o in test_sevens]\n","stacked_test_sevens_tensor = torch.stack(test_sevens_tensor).float()/255\n","\n","#stacked_test_sevens_tensor.shape\n","\n","#mean1 = stacked_test_ones_tensor.mean(0)\n","#show_image(mean1);\n","\n","#mean7 = stacked_test_sevens_tensor.mean(0)\n","#show_image(mean7)\n"],"metadata":{"id":"nU2vJE-IvNyk","executionInfo":{"status":"ok","timestamp":1681882461667,"user_tz":-120,"elapsed":31581,"user":{"displayName":"David Cloos","userId":"01050300783638368919"}},"colab":{"base_uri":"https://localhost:8080/","height":37},"outputId":"41e3460a-f342-44da-8c8e-fff5f09de71e"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='15687680' class='' max='15683414' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.03% [15687680/15683414 00:00&lt;00:00]\n","    </div>\n","    "]},"metadata":{}}]},{"cell_type":"markdown","source":["*2) Instead of using an optimal 1 or 7 with the mean over the training dataset, try to calculate the sum of the distances to all instances in the training set for each instance in the test dataset. You can use the L2 norm.*"],"metadata":{"id":"Zb6imBzf4U2k"}},{"cell_type":"code","source":["#2)try to calculate the sum of the distances to all instances in the training set for each instance in the test dataset. You can use the L2 norm.\n","\n","def mnist_distance(a,b): return F.mse_loss(a,b).sqrt()\n"],"metadata":{"id":"-FyVypAz4FjL","executionInfo":{"status":"ok","timestamp":1681882461669,"user_tz":-120,"elapsed":25,"user":{"displayName":"David Cloos","userId":"01050300783638368919"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["3) For each instance in the test set, decide if it is a 1 or 7 and calculate the precision"],"metadata":{"id":"IfV7TeUGAh1I"}},{"cell_type":"code","source":["def is_1(x): return mnist_distance(x,stacked_train_ones_tensor) < mnist_distance(x,stacked_train_sevens_tensor)\n","\n","#a_1 = stacked_test_ones_tensor[1]\n","#is_1(a_1), is_1(a_1).float()\n","\n","counter_one=0\n","counter_seven=0\n","\n","for o in stacked_test_ones_tensor:\n","  if is_1(o):\n","    counter_one+=1\n","\n","for s in stacked_test_sevens_tensor:\n","  if not is_1(s):\n","    counter_seven+=1\n","\n","accuracy_one = counter_one / stacked_test_ones_tensor.shape[0]\n","accuracy_seven = counter_seven / stacked_test_sevens_tensor.shape[0]\n","\n","accuracy_one,accuracy_seven,(accuracy_one+accuracy_seven)/2\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLpUgEO1Aj5Y","executionInfo":{"status":"ok","timestamp":1681882528683,"user_tz":-120,"elapsed":67036,"user":{"displayName":"David Cloos","userId":"01050300783638368919"}},"outputId":"7b516e21-f5a5-4356-9536-0a48e36324ba"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-7ff6e0a31fc8>:3: UserWarning: Using a target size (torch.Size([6301, 28, 28])) that is different to the input size (torch.Size([28, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  def mnist_distance(a,b): return F.mse_loss(a,b).sqrt()\n","<ipython-input-5-7ff6e0a31fc8>:3: UserWarning: Using a target size (torch.Size([5834, 28, 28])) that is different to the input size (torch.Size([28, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  def mnist_distance(a,b): return F.mse_loss(a,b).sqrt()\n"]},{"output_type":"execute_result","data":{"text/plain":["(1.0, 0.7827278958190541, 0.891363947909527)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["print(\"Sevens Tensor\" , stacked_test_sevens_tensor.shape[0])\n","print(\"Counter\", counter_seven)\n","\n","print(\"Ones Tensor\" , stacked_test_ones_tensor.shape[0])\n","print(\"Counter\", counter_one)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zLdMpMpzsqSC","executionInfo":{"status":"ok","timestamp":1681882528684,"user_tz":-120,"elapsed":73,"user":{"displayName":"David Cloos","userId":"01050300783638368919"}},"outputId":"b5c29765-7735-43a3-ee9f-9c32d466dd49"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Sevens Tensor 1459\n","Counter 1142\n","Ones Tensor 1576\n","Counter 1576\n"]}]},{"cell_type":"markdown","source":["****"],"metadata":{"id":"pe6s0MO4YJsF"}},{"cell_type":"markdown","source":["# Stochastic Gradient Descent (SGD)\n","\n","For this exercise I ask you to read the chapter Stochastic Gradient Descent (SGD) from the Google Colab 04_mnist_basics.ipynb in paralell. The chapter starts with a single TLU, compare p. 304 in \"Hands on Machine Learning\". Go through all 7 steps which are an easy example of how Stochastic Gradient Descent works.\n","\n","Our goal is to train a single TLU, which can decide if one number is larger then the other one. Therefore we create 100 random pairs with pyTorch and create a target vector which is eather 1 or 0.\n"],"metadata":{"id":"ETcE9B9rdcEI"}},{"cell_type":"code","source":["x = torch.randn((100, 2))\n","y = torch.where(x[:,0] > x[:,1], 1.0, 0.0)\n","print(x[0:10])\n","print(\"y\" , y[0:10])"],"metadata":{"id":"17qLyDnbpSbB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681882528685,"user_tz":-120,"elapsed":29,"user":{"displayName":"David Cloos","userId":"01050300783638368919"}},"outputId":"081a1f5b-0321-4f6b-c9d7-8cc190dae11d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.9269,  1.4873],\n","        [ 0.9007, -2.1055],\n","        [ 0.6784, -1.2345],\n","        [-0.0431, -1.6047],\n","        [-0.7521,  1.6487],\n","        [-0.3925, -1.4036],\n","        [-0.7279, -0.5594],\n","        [-0.7688,  0.7624],\n","        [ 1.6423, -0.1596],\n","        [-0.4974,  0.4396]])\n","y tensor([1., 1., 1., 1., 0., 1., 0., 0., 1., 0.])\n"]}]},{"cell_type":"code","source":["print(x[:,0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GH_MgWzbSrG2","executionInfo":{"status":"ok","timestamp":1681882528685,"user_tz":-120,"elapsed":25,"user":{"displayName":"David Cloos","userId":"01050300783638368919"}},"outputId":"be5d6240-cdf8-4be5-c9a7-854223dc4203"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 1.9269e+00,  9.0072e-01,  6.7842e-01, -4.3067e-02, -7.5214e-01, -3.9248e-01, -7.2788e-01, -7.6884e-01,  1.6423e+00, -4.9740e-01, -7.5813e-01,  8.0080e-01,  1.2791e+00,  6.1047e-01,\n","        -2.3162e-01, -2.5158e-01, -1.3847e+00, -2.2337e-01,  3.1888e-01,  3.0572e-01, -1.5576e+00, -8.7979e-01, -1.2742e+00, -1.2347e+00, -9.1382e-01,  7.8024e-02, -4.8799e-01, -8.1401e-01,\n","        -1.4032e+00, -6.3477e-02, -9.7807e-02, -1.1845e+00,  1.4451e+00,  2.2181e+00,  3.4665e-01, -1.0546e+00, -1.7219e-01,  5.6622e-02,  5.7501e-01, -2.2064e+00,  1.0868e-02, -1.3407e+00,\n","         5.3619e-01,  1.1412e+00,  7.4395e-01, -1.0495e+00, -1.7223e+00,  1.3347e+00, -2.5095e+00,  7.8459e-01,  6.4076e-01,  1.0669e+00, -1.8527e-01,  4.0476e-01,  2.6491e-01, -1.3109e-03,\n","        -1.4570e+00, -5.9915e-01,  7.2618e-01, -3.8907e-01, -1.2685e-02,  1.3254e-01,  1.0950e+00,  7.1997e-01,  1.9312e+00, -1.4364e+00, -1.3603e-01,  6.5474e-01,  1.1415e+00, -1.8058e+00,\n","        -3.7534e-01, -6.8665e-01, -9.7267e-01,  1.6192e+00,  2.6948e-01, -7.3280e-01,  3.4875e-01, -4.6569e-01, -2.4801e+00, -1.1955e+00, -1.9006e+00,  2.4859e-02,  2.8683e-01,  1.7482e-01,\n","        -1.6022e+00,  1.2888e+00, -1.5469e+00,  7.7552e-01,  3.5818e-02, -8.0566e-01, -9.3195e-01, -1.1360e+00, -1.5933e-01,  9.4423e-01,  1.0608e+00, -5.7785e-01,  2.6178e-01, -2.0461e+00,\n","         4.0487e-01,  3.1253e-01])\n"]}]},{"cell_type":"markdown","source":["Your task is to create a function f that is a single TLU, meaning that it summarizes x with weights a, b, c:\n","\n","$ax_0+bx_1+c$\n","\n","In Addition we are using a *sigmoid()* function as step function.\n","\n","$f = \\text{sigmoid}(ax_0+bx_1+c)$"],"metadata":{"id":"z267w4G48rxp"}},{"cell_type":"code","source":["def f(x, params):\n","    a,b,c = params\n","    return sigmoid(a*x[:,0] + b*x[:,1]+ c) \n","\n","print(f(x, [3,-2,1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_NvBnCGoLPx","executionInfo":{"status":"ok","timestamp":1681882528686,"user_tz":-120,"elapsed":22,"user":{"displayName":"David Cloos","userId":"01050300783638368919"}},"outputId":"a2cdb619-22f9-4f13-8874-c5405bfee5be"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([9.7825e-01, 9.9963e-01, 9.9595e-01, 9.8338e-01, 1.0417e-02, 9.3275e-01, 4.8381e-01, 5.5650e-02, 9.9807e-01, 2.0240e-01, 3.1340e-02, 5.1029e-01, 9.0418e-01, 5.4039e-01, 5.5518e-01, 1.8627e-01,\n","        1.9599e-01, 4.2909e-02, 9.4298e-01, 9.6971e-01, 3.4567e-03, 3.9244e-01, 8.5119e-04, 1.5083e-01, 3.9527e-01, 5.4549e-01, 5.4851e-02, 5.0749e-01, 3.6203e-02, 3.6780e-01, 4.8218e-02, 4.8655e-03,\n","        9.7398e-01, 9.9865e-01, 9.1943e-01, 8.8385e-03, 3.6259e-01, 5.7866e-01, 9.8216e-01, 1.6023e-02, 8.4685e-01, 1.3572e-01, 8.2626e-01, 9.8688e-01, 9.8515e-01, 3.3693e-02, 7.5066e-02, 9.8266e-01,\n","        5.5022e-04, 9.6431e-01, 8.5268e-01, 9.9395e-01, 2.5706e-01, 8.6499e-01, 3.2047e-01, 8.3248e-01, 4.0449e-02, 1.4785e-01, 9.5240e-01, 2.2740e-01, 6.1781e-01, 4.6733e-01, 9.7354e-01, 9.1190e-01,\n","        9.9159e-01, 2.5932e-01, 6.4229e-02, 8.5963e-01, 9.8772e-01, 1.8919e-03, 1.0045e-01, 8.8380e-02, 2.1146e-02, 9.5056e-01, 9.0284e-01, 1.9671e-01, 5.2774e-01, 2.6425e-02, 3.6649e-03, 1.4614e-02,\n","        5.7157e-03, 8.5402e-01, 9.6518e-01, 9.7616e-01, 1.4829e-03, 9.9152e-01, 5.7433e-03, 3.2596e-01, 7.0397e-01, 2.6858e-01, 7.9997e-01, 2.0380e-01, 7.9769e-01, 9.8526e-01, 9.7737e-01, 2.0029e-01,\n","        9.6461e-01, 1.1110e-01, 7.2129e-01, 8.8128e-01])\n"]}]},{"cell_type":"markdown","source":["In addition to our TLU function, we need a loss function. Your task is to implement a absolute difference loss function, $∑|x_i-y_i|$, which counts the number of wrong guesses."],"metadata":{"id":"UBiKkGKx-jVM"}},{"cell_type":"code","source":["def mae(preds, targets): return (preds-targets.abs()).sum()"],"metadata":{"id":"cwzyy281wI7Q","executionInfo":{"status":"ok","timestamp":1681882528686,"user_tz":-120,"elapsed":17,"user":{"displayName":"David Cloos","userId":"01050300783638368919"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Try to train your single TLU with the absolute difference loss function, use the following code. Choose an appropriate step weight `lr` and try to explain what is happing in each line."],"metadata":{"id":"eGVNErmbvFxB"}},{"cell_type":"code","source":["lr = 1\n","#Initializes a Torch Tensor with 3 random parameters\n","params = torch.randn(3).requires_grad_()\n","\n","def apply_step(params, prn=True):\n","    #Calculate predictions\n","    preds = f(x, params)\n","    #Loss function with mean absolute error of predictions and y \n","    loss = mae(preds, y)\n","    # Perform back propagation with predicted values --> Calculate gradients \n","    # that define how the param need to change\n","    loss.backward()\n","    #Based on Back propagation -> update parameters\n","    params.data -= lr * params.grad.data\n","    params.grad = None\n","    #if prn=true -> print parameter and loss\n","    if prn: \n","        print(params);print(loss.item())\n","    return preds\n","\n","\n","for i in range(10): apply_step(params)"],"metadata":{"id":"EB5TYTNmyO3d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681882528687,"user_tz":-120,"elapsed":17,"user":{"displayName":"David Cloos","userId":"01050300783638368919"}},"outputId":"7defe3a1-90f1-4667-94b7-596fb265c1f5"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([  3.5752,  -4.1180, -17.5908], requires_grad=True)\n","0.8166449069976807\n","tensor([  3.5721,  -4.1109, -17.5942], requires_grad=True)\n","-39.99654769897461\n","tensor([  3.5691,  -4.1040, -17.5976], requires_grad=True)\n","-39.99661636352539\n","tensor([  3.5661,  -4.0972, -17.6009], requires_grad=True)\n","-39.99668502807617\n","tensor([  3.5632,  -4.0905, -17.6041], requires_grad=True)\n","-39.99674987792969\n","tensor([  3.5604,  -4.0840, -17.6073], requires_grad=True)\n","-39.9968147277832\n","tensor([  3.5576,  -4.0776, -17.6104], requires_grad=True)\n","-39.99687576293945\n","tensor([  3.5549,  -4.0713, -17.6134], requires_grad=True)\n","-39.9969367980957\n","tensor([  3.5522,  -4.0651, -17.6164], requires_grad=True)\n","-39.99699020385742\n","tensor([  3.5495,  -4.0590, -17.6194], requires_grad=True)\n","-39.99704360961914\n"]}]},{"cell_type":"markdown","source":["Write a line of code that counts the number of wrong predictions, rounding your predictions with *round()*."],"metadata":{"id":"h5_LNc1o_o2g"}},{"cell_type":"code","source":["preds = f(x, params)\n","wrong_preds = ((preds.round() - y).abs()).sum()\n","wrong_preds\n"],"metadata":{"id":"EEUhyhyDxwMQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681882528687,"user_tz":-120,"elapsed":13,"user":{"displayName":"David Cloos","userId":"01050300783638368919"}},"outputId":"e1ebbe1c-1433-4e23-decc-7fbd9435d0b5"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(40., grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":14}]}]}